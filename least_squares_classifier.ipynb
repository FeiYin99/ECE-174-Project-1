{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 1 Problem 1: Least Squares Classifier #\n",
    "### Name: Fei Yin ### \n",
    "### PID: A15555426 ###\n",
    "\n",
    "In this problem, we will first solve a least squares problem. Using its solution, we will build a binary classifier, a one-versus-all classifier, and a one-versus-one classifier to classify hand-written digits. Finally, we will evaluate the classifiers' performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from numpy.linalg import pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data from mnist.mat\n",
    "mnist = loadmat('mnist.mat')\n",
    "train_x = mnist['trainX']\n",
    "train_y = mnist['trainY']\n",
    "test_x = mnist['testX']\n",
    "test_y = mnist['testY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter out pixels with nonzero values in < 600 training samples\n",
    "pixel_nonzero_count = np.zeros(28 * 28)\n",
    "\n",
    "for image in train_x:\n",
    "    pixel_nonzero_count = np.where(image != 0, pixel_nonzero_count + 1, pixel_nonzero_count)\n",
    "\n",
    "train_x = train_x[:, pixel_nonzero_count >= 600]\n",
    "test_x = test_x[:, pixel_nonzero_count >= 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 493)\n",
      "(1, 60000)\n",
      "(10000, 493)\n",
      "(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "## Verify shapes of training and test data\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Least Squares Problem ##\n",
    "To build the binary classifier, one-versus-all classifier, and one-versus-one classifier, we must first solve the least squares problem, which is given as $\\min_{\\beta, \\alpha}\\sum_{i=1}^N (y_i - \\beta^T x_i - \\alpha)^2$. This is equivalent to $\\min_{\\beta, \\alpha} ||y - \\beta^T x - \\mathrm{A}||^2$ where $\\mathrm{A}$ is a list of $\\alpha$'s and has the same shape as $y$. \n",
    "\n",
    "Before doing anything else, we first want to check the dimension of this problem. Since the given y (`train_y`) has shape (1, 60000), x (`train_x`) has shape (60000, 493), and we want $\\beta$ to have 493 elements, we need to transpose x to get it into shape (493, 60000) where $y - \\beta^T x^T$ could have the correct dimension.\n",
    "\n",
    "In order to convert this problem into the form $\\min_{x} ||y - Ax||^2$, which we can solve by solving the normal equation, we would have to 1) absorb $\\mathrm{A}$ into $\\beta^T x^T$ and 2) reverse the order of $\\beta^T$ and $x^T$ because in our problem, $x^T$ is the feature vector, and $\\beta^T$ is the coefficient vector we want to solve for. \n",
    "\n",
    "1) To absorb $\\mathrm{A}$ into $\\beta^T x^T$, we can add $\\alpha$ to the end of $\\beta$, making $\\beta$ a 494-vector, then add a column of 1's to x, such that the problem $y_i - \\beta^T x_i - \\alpha$ can be preserved.\n",
    "\n",
    "2) To reverse the order of $\\beta^T$ and $x^T$ in $\\beta^T x^T$, we first convert $\\beta^T x^T$ to $(x\\beta)^T$. Then, we convert $y - (x\\beta)^T$ to $(y^T - x\\beta)^T$, which are all done by utilizing the identities of tranpose. Note that the outer-most transpose can be ignored because it does not interfere with the minimization problem.\n",
    "\n",
    "Finally, we have our problem in the form $\\min_{\\beta} ||y^T - x\\beta||^2$, which we can solve by calculating $\\beta = (x^T x)^{-1} x^T y^T$. This is implemented in the `solveLeastSquares` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveLeastSquares(features, labels):\n",
    "    \"\"\"Solve a least squares problem.\n",
    "\n",
    "    Keyword arguments:\n",
    "    features -- a (m x n) matrix with m features each as an n-vector\n",
    "    labels -- a (1 x m) matrix with m labels \n",
    "    \n",
    "    Returns:\n",
    "    B -- a (n x 1) matrix with n trained coefficients \n",
    "    a -- a number representing the trained bias\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Add a column of ones to features to account for the bias term a\n",
    "    features_new = np.column_stack((features, np.ones(features.shape[0])))\n",
    "    \n",
    "    ## Solve the least square problem to get the minimizer B and bias term a\n",
    "    B_and_a = pinv(features_new.T @ features_new) @ features_new.T @ labels.T\n",
    "    B = B_and_a[0:-1]\n",
    "    a = B_and_a[-1]\n",
    "    \n",
    "    return B, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `solveLeastSquares`, we can now implement the `binaryClassifier` and `oneVersusAllClassifier` functions. The least squares problems are always solved with training features and training labels whereas predictions are made with training features in the case of training or testing features in the case of testing. **Note that the functions assume testing case if testing features are included in the functions' parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryClassifier(train_features, train_labels, test_features=None):\n",
    "    \"\"\"A binary classifier that classifies the features into two categories.\n",
    "\n",
    "    Keyword arguments:\n",
    "    train_features -- a (m x n) matrix with m training features each as an n-vector\n",
    "    train_labels -- a (1 x m) matrix with m training labels\n",
    "    test_features -- a (k x n) matrix with k testing features each as an n-vector\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- a (1 x m) matrix with m predictions in the case of training \n",
    "                   or a (1 x k) matrix with k predictions in the case of testing \n",
    "    \"\"\"\n",
    "    \n",
    "    B, a = solveLeastSquares(train_features, train_labels)\n",
    "    \n",
    "    ## Classify the results\n",
    "    if test_features is None:\n",
    "        predictions = np.sign(B.T @ train_features.T + a)\n",
    "    else:\n",
    "        predictions = np.sign(B.T @ test_features.T + a)\n",
    "     \n",
    "    predictions[predictions == 0] = 1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVersusAllClassifier(train_features, train_labels, test_features=None):\n",
    "    \"\"\"A one-versus-all classifier that classifies the features into 10 categories.\n",
    "\n",
    "    Keyword arguments:\n",
    "    train_features -- a (m x n) matrix with m training features each as an n-vector\n",
    "    train_labels -- a (1 x m) matrix with m training labels\n",
    "    test_features -- a (k x n) matrix with k testing features each as an n-vector\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- a (1 x m) matrix with m predictions in the case of training \n",
    "                   or a (1 x k) matrix with k predictions in the case of testing \n",
    "    \"\"\"\n",
    "    \n",
    "    is_training = (test_features is None)\n",
    "    \n",
    "    if is_training:\n",
    "        g_max = np.full((1, train_features.shape[0]), np.NINF)\n",
    "        predictions = np.zeros((1, train_features.shape[0]))\n",
    "    else:\n",
    "        g_max = np.full((1, test_features.shape[0]), np.NINF)\n",
    "        predictions = np.zeros((1, test_features.shape[0]))\n",
    "        \n",
    "    \n",
    "    for digit in range(0, 10):\n",
    "        \n",
    "        ## Create a new set of labels for each digit\n",
    "        train_labels_new = np.copy(train_labels[0]).astype(float)\n",
    "        train_labels_new = np.where(train_labels_new == digit, 1, -1)\n",
    "        train_labels_new = np.reshape(train_labels_new, (1, len(train_labels_new)))\n",
    "        \n",
    "        B, a = solveLeastSquares(train_features, train_labels_new)\n",
    "\n",
    "        ## Calculate g and classify each feature as the digit corresponding to the largest g\n",
    "        if is_training:\n",
    "            g = B.T @ train_features.T + a\n",
    "        else:\n",
    "            g = B.T @ test_features.T + a\n",
    "            \n",
    "        predictions = np.where(g > g_max, digit, predictions)\n",
    "        g_max = np.where(g > g_max, g, g_max)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `oneVersusOneClassifier` function below depends on the `binaryClassifier` function. We apply `binaryClassifier` once for each pair of digits. However, note that although the coefficients are obtained with pruned sets of training features and training labels, the trained coefficients are not applied to only the pruned training_features, but all 60000 training features in the case of training or all 10000 testing features in the case of testing.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVersusOneClassifier(train_features, train_labels, test_features=None):\n",
    "    \"\"\"A one-versus-one classifier that classifies the features into 10 categories.\n",
    "           This function uses the binaryClassifier function. \n",
    "\n",
    "    Keyword arguments:\n",
    "    train_features -- a (m x n) matrix with m training features each as an n-vector\n",
    "    train_labels -- a (1 x m) matrix with m training labels\n",
    "    test_features -- a (k x n) matrix with k testing features each as an n-vector\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- a (1 x m) matrix with m predictions in the case of training \n",
    "                   or a (1 x k) matrix with k predictions in the case of testing \n",
    "    \"\"\"\n",
    "    \n",
    "    is_training = (test_features is None)\n",
    "    \n",
    "    if is_training:\n",
    "        votes = np.zeros((train_features.shape[0], 10))\n",
    "    else:\n",
    "        votes = np.zeros((test_features.shape[0], 10))\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(i + 1, 10):\n",
    "                        \n",
    "            if is_training:\n",
    "                used_indices = np.arange(train_features.shape[0])\n",
    "            \n",
    "            else:\n",
    "                used_indices = np.arange(test_features.shape[0])\n",
    "                \n",
    "            ## Create copies of training features and training labels to be pruned\n",
    "            train_features_new = np.copy(train_features)\n",
    "            train_labels_new = np.copy(train_labels[0]).astype(float)\n",
    "\n",
    "            ## Remove the features and labels that do not correspond to the current pair of digits\n",
    "            ##   while keeping track of their original indices\n",
    "            which_entries_to_keep = np.logical_or(train_labels_new == i, train_labels_new == j)\n",
    "            train_features_new = train_features_new[which_entries_to_keep]\n",
    "            train_labels_new = train_labels_new[which_entries_to_keep]\n",
    "                \n",
    "            ## Modify the training labels according to the current pair of digits\n",
    "            train_labels_new[train_labels_new == i] = 1\n",
    "            train_labels_new[train_labels_new == j] = -1\n",
    "            train_labels_new = np.reshape(train_labels_new, (1, len(train_labels_new)))\n",
    "            \n",
    "            if is_training:\n",
    "                ## In the case of training, the original training features are passed as testing features into\n",
    "                ##   binaryClassifier so that the trained coefficients are applied to all training features \n",
    "                predictions = binaryClassifier(train_features_new, train_labels_new, train_features)\n",
    "            else:\n",
    "                predictions = binaryClassifier(train_features_new, train_labels_new, test_features)\n",
    "                \n",
    "            ## Track the votes for each digit\n",
    "            for k, used_index in enumerate(used_indices):\n",
    "                if predictions[0][k] == 1:\n",
    "                    votes[used_index][i] += 1\n",
    "                elif predictions[0][k] == -1:\n",
    "                    votes[used_index][j] += 1\n",
    "\n",
    "    return np.argmax(votes, axis=-1).reshape((1, votes.shape[0]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the performances of the classifiers, confusion matrices can be built with the `buildConfusionMatrix` function. The error rate of a confusion matrix can be calculated with the `calculateErrorRate` function given a confusion matrix as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildConfusionMatrix(labels, predictions):\n",
    "    \"\"\"Build an 11 x 11 confusion matrix for the results of oneVersusAllClassifier and oneVersusOneClassifier.\n",
    "\n",
    "    Keyword arguments:\n",
    "    labels -- a (1 x m) matrix with each entry representing a label\n",
    "    predictions -- a (1 x m) matrix with each entry representing a prediction\n",
    "    \n",
    "    Returns:\n",
    "    confusion -- a (11 x 11) confusion matrix \n",
    "    \"\"\"\n",
    "    \n",
    "    confusion = np.zeros((11, 11))\n",
    "\n",
    "    for label, prediction in zip(labels[0], predictions[0]):        \n",
    "        confusion[int(label), int(prediction)] += 1\n",
    "    \n",
    "    for i in range(10):\n",
    "        confusion[i, -1] = np.sum(confusion[i, 0:10])\n",
    "        confusion[-1, i] = np.sum(confusion[0:10, i])\n",
    "    \n",
    "    confusion[-1, -1] = labels.shape[1]        \n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateErrorRate(confusion):\n",
    "    \"\"\"Calculate the error rate based on a confusion matrix\n",
    "\n",
    "    Keyword arguments:\n",
    "    confusion -- a (k x k) confusion matrix, where k-1 is the number of classified categories\n",
    "    \n",
    "    Returns:\n",
    "    error_rates -- The error rates of the classification on individual digits\n",
    "                   and the average error rate of the classification on all digits\n",
    "    \"\"\"\n",
    "    \n",
    "    error_rates = []\n",
    "    true_positive_count = 0\n",
    "    class_count = confusion.shape[0] - 1\n",
    "    \n",
    "    for i in range(class_count):\n",
    "        error_rates.append((confusion[i, -1] - confusion[i, i]) / confusion[i, -1])\n",
    "        true_positive_count += confusion[i, i]\n",
    "        \n",
    "    error_rates.append((confusion[-1, -1] - true_positive_count) / confusion[-1, -1])    \n",
    "    return error_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-versus-all Classifier on Training Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run one-versus-all classifier on the training data\n",
    "one_v_all_predictions = oneVersusAllClassifier(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5669.     8.    21.    19.    25.    46.    65.     4.    60.     6.  5923.]\n",
      " [    2.  6543.    36.    17.    20.    30.    14.    14.    60.     6.  6742.]\n",
      " [   99.   278.  4757.   153.   116.    17.   234.    92.   190.    22.  5958.]\n",
      " [   38.   172.   174.  5150.    31.   122.    59.   122.   135.   128.  6131.]\n",
      " [   13.   104.    41.     5.  5189.    52.    45.    24.    60.   309.  5842.]\n",
      " [  164.    94.    30.   448.   103.  3974.   185.    44.   237.   142.  5421.]\n",
      " [  104.    78.    77.     2.    64.   106.  5448.     0.    36.     3.  5918.]\n",
      " [   55.   191.    36.    48.   165.     9.     4.  5443.    13.   301.  6265.]\n",
      " [   69.   492.    64.   225.   102.   220.    64.    21.  4417.   177.  5851.]\n",
      " [   67.    66.    26.   115.   365.    12.     4.   513.    39.  4742.  5949.]\n",
      " [ 6280.  8026.  5262.  6182.  6180.  4588.  6122.  6277.  5247.  5836. 60000.]]\n"
     ]
    }
   ],
   "source": [
    "## Build confusion matrix for the results to the one-versus-all classifier\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions( linewidth=100)\n",
    "one_v_all_confusion = buildConfusionMatrix(train_y, one_v_all_predictions)\n",
    "print(one_v_all_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for  0 :  0.04288367381394564\n",
      "Error rate for  1 :  0.029516463957282704\n",
      "Error rate for  2 :  0.20157771064115476\n",
      "Error rate for  3 :  0.1600065242211711\n",
      "Error rate for  4 :  0.11177678877096885\n",
      "Error rate for  5 :  0.2669249216011806\n",
      "Error rate for  6 :  0.07941872254139912\n",
      "Error rate for  7 :  0.13120510774142058\n",
      "Error rate for  8 :  0.24508631003247308\n",
      "Error rate for  9 :  0.20289124222558413\n",
      "Average error rate:  0.14446666666666666\n"
     ]
    }
   ],
   "source": [
    "## Calculate the error rates from the confusion matrix\n",
    "one_v_all_error_rates = calculateErrorRate(one_v_all_confusion)\n",
    "for i, error_rate in enumerate(one_v_all_error_rates[:-1]):\n",
    "    print('Error rate for ', i, ': ', error_rate)\n",
    "else:\n",
    "    print('Average error rate: ', one_v_all_error_rates[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The one-versus-one classifier identifies the hand-written digits with about 14.4% total error rate on the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-versus-one Classifier on Training Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run one-versus-one classifier on the training data\n",
    "one_v_one_predictions = oneVersusOneClassifier(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5756.     3.    22.    13.    16.    34.    31.     7.    40.     1.  5923.]\n",
      " [    1.  6624.    37.    17.     4.    15.     3.    12.    22.     7.  6742.]\n",
      " [   26.    81.  5504.    52.    67.    32.    46.    47.    86.    17.  5958.]\n",
      " [   17.    44.   124.  5557.     7.   176.    23.    49.    93.    41.  6131.]\n",
      " [   11.    20.    15.     2.  5575.    10.    17.    19.     6.   167.  5842.]\n",
      " [   38.    43.    43.   141.    26.  4968.    92.     8.    45.    17.  5421.]\n",
      " [   17.    17.    43.     2.    36.    89.  5685.     0.    28.     1.  5918.]\n",
      " [    4.    81.    56.     5.    82.    14.     1.  5862.     6.   154.  6265.]\n",
      " [   16.   200.    50.   114.    45.   148.    38.    30.  5143.    67.  5851.]\n",
      " [   14.    14.    23.    81.   164.    37.     2.   154.    32.  5428.  5949.]\n",
      " [ 5900.  7127.  5917.  5984.  6022.  5523.  5938.  6188.  5501.  5900. 60000.]]\n"
     ]
    }
   ],
   "source": [
    "## Build confusion matrix for the results to the one-versus-one classifier\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions( linewidth=100)\n",
    "one_v_one_confusion = buildConfusionMatrix(train_y, one_v_one_predictions)\n",
    "print(one_v_one_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for  0 :  0.028195171365861894\n",
      "Error rate for  1 :  0.017502224859092256\n",
      "Error rate for  2 :  0.07620006713662303\n",
      "Error rate for  3 :  0.093622573805252\n",
      "Error rate for  4 :  0.04570352618966107\n",
      "Error rate for  5 :  0.0835639180962922\n",
      "Error rate for  6 :  0.039371409259885096\n",
      "Error rate for  7 :  0.06432561851556265\n",
      "Error rate for  8 :  0.12100495641770638\n",
      "Error rate for  9 :  0.08757774415868214\n",
      "Average error rate:  0.06496666666666667\n"
     ]
    }
   ],
   "source": [
    "## Calculate the error rates from the confusion matrix\n",
    "one_v_one_error_rates = calculateErrorRate(one_v_one_confusion)\n",
    "for i, error_rate in enumerate(one_v_one_error_rates[:-1]):\n",
    "    print('Error rate for ', i, ': ', error_rate)\n",
    "else:\n",
    "    print('Average error rate: ', one_v_one_error_rates[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The one-versus-one classifier identifies the hand-written digits with about 6.5% total error rate on the training data. This is about half of the error rate of the one-versus-all classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-versus-all Classifier on Testing Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run one-versus-all classifier on the testing data\n",
    "one_v_all_predictions = oneVersusAllClassifier(train_x, train_y, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  944.     0.     1.     2.     2.     8.    13.     2.     7.     1.   980.]\n",
      " [    0.  1107.     2.     2.     3.     1.     5.     1.    14.     0.  1135.]\n",
      " [   18.    54.   815.    26.    16.     0.    38.    22.    39.     4.  1032.]\n",
      " [    4.    18.    22.   884.     5.    16.    10.    22.    20.     9.  1010.]\n",
      " [    0.    22.     6.     0.   883.     3.     9.     1.    12.    46.   982.]\n",
      " [   24.    19.     3.    74.    24.   656.    24.    13.    38.    17.   892.]\n",
      " [   17.     9.    10.     0.    22.    17.   876.     0.     7.     0.   958.]\n",
      " [    5.    43.    14.     6.    25.     1.     1.   883.     1.    49.  1028.]\n",
      " [   14.    48.    11.    31.    26.    40.    17.    13.   756.    18.   974.]\n",
      " [   16.    10.     3.    17.    80.     0.     1.    75.     4.   803.  1009.]\n",
      " [ 1042.  1330.   887.  1042.  1086.   742.   994.  1032.   898.   947. 10000.]]\n"
     ]
    }
   ],
   "source": [
    "## Build confusion matrix for the results to the one-versus-all classifier\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions( linewidth=100)\n",
    "one_v_all_confusion = buildConfusionMatrix(test_y, one_v_all_predictions)\n",
    "print(one_v_all_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for  0 :  0.036734693877551024\n",
      "Error rate for  1 :  0.024669603524229075\n",
      "Error rate for  2 :  0.21027131782945738\n",
      "Error rate for  3 :  0.12475247524752475\n",
      "Error rate for  4 :  0.10081466395112017\n",
      "Error rate for  5 :  0.2645739910313901\n",
      "Error rate for  6 :  0.08559498956158663\n",
      "Error rate for  7 :  0.14105058365758755\n",
      "Error rate for  8 :  0.22381930184804927\n",
      "Error rate for  9 :  0.20416253716551042\n",
      "Average error rate:  0.1393\n"
     ]
    }
   ],
   "source": [
    "## Calculate the error rates from the confusion matrix\n",
    "one_v_all_error_rates = calculateErrorRate(one_v_all_confusion)\n",
    "for i, error_rate in enumerate(one_v_all_error_rates[:-1]):\n",
    "    print('Error rate for ', i, ': ', error_rate)\n",
    "else:\n",
    "    print('Average error rate: ', one_v_all_error_rates[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The one-versus-all classifier identifies the hand-written digits with about 14.0% total error rate on the testing data. This is consistent with its performance on the training data, which means the one-versus-all classifier generalizes on the testing data very well. Among its performances on various digits, \"1\" is the easiest to recognize with about 2.5% error rate whereas \"5\" is the hardest to recognize with about 26.5% error rate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-versus-one Classifier on Testing Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run one-versus-one classifier on the testing data\n",
    "one_v_one_predictions = oneVersusOneClassifier(train_x, train_y, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  949.     0.     4.     2.     2.     8.     8.     4.     2.     1.   980.]\n",
      " [    0.  1122.     3.     3.     1.     1.     2.     1.     2.     0.  1135.]\n",
      " [    6.    22.   931.    14.    13.     5.    11.     8.    22.     0.  1032.]\n",
      " [    4.     1.    19.   927.     3.    20.     3.    10.    18.     5.  1010.]\n",
      " [    0.     2.     7.     1.   934.     3.     6.     3.     2.    24.   982.]\n",
      " [    8.     6.     2.    30.     9.   802.    16.     2.    13.     4.   892.]\n",
      " [    6.     6.    10.     0.     8.    20.   906.     0.     2.     0.   958.]\n",
      " [    1.    17.    17.     3.    10.     1.     0.   955.     2.    22.  1028.]\n",
      " [    5.    16.    10.    21.    11.    36.    12.    10.   840.    13.   974.]\n",
      " [    4.     5.     1.    10.    31.    12.     1.    23.     5.   917.  1009.]\n",
      " [  983.  1197.  1004.  1011.  1022.   908.   965.  1016.   908.   986. 10000.]]\n"
     ]
    }
   ],
   "source": [
    "## Build confusion matrix for the results to the one-versus-one classifier\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions( linewidth=100)\n",
    "one_v_one_confusion = buildConfusionMatrix(test_y, one_v_one_predictions)\n",
    "print(one_v_one_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for  0 :  0.03163265306122449\n",
      "Error rate for  1 :  0.01145374449339207\n",
      "Error rate for  2 :  0.09786821705426356\n",
      "Error rate for  3 :  0.08217821782178218\n",
      "Error rate for  4 :  0.048879837067209775\n",
      "Error rate for  5 :  0.10089686098654709\n",
      "Error rate for  6 :  0.054279749478079335\n",
      "Error rate for  7 :  0.07101167315175097\n",
      "Error rate for  8 :  0.1375770020533881\n",
      "Error rate for  9 :  0.09117938553022795\n",
      "Average error rate:  0.0717\n"
     ]
    }
   ],
   "source": [
    "## Calculate the error rates from the confusion matrix\n",
    "one_v_one_error_rates = calculateErrorRate(one_v_one_confusion)\n",
    "for i, error_rate in enumerate(one_v_one_error_rates[:-1]):\n",
    "    print('Error rate for ', i, ': ', error_rate)\n",
    "else:\n",
    "    print('Average error rate: ', one_v_one_error_rates[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The one-versus-one classifier identifies the hand-written digits with about 7.2% total error rate on the testing data. This is consistent with its performance on the training data, which means the one-versus-one classifier also generalizes well on the testing data. Among its performances on various digits, \"1\" is the easiest to recognize with about 1.1% error rate and \"8\" is the hardest to recognize with about 13.8% error rate, which shows that the one-versus-one classifier behaves differently than the one-versus-all classifier.**\n",
    "\n",
    "**Similar to their performances on the training data, the one-versus-one classifier's total error rate on the testing data is approximately half of the one-versus-all classifier's total error rate on the testing data.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
